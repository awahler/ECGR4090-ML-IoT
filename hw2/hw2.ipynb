{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interested-lambda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL \n",
    "from PIL import Image\n",
    "from keras_preprocessing import image\n",
    "\n",
    "from numpy import asarray\n",
    "\n",
    "from tensorflow.keras import Input, layers\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chinese-allowance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images range from 0.00000 to 1.00000\n",
      "Test     Images range from 0.00000 to 1.00000\n"
     ]
    }
   ],
   "source": [
    "dataset = \"cifar\"\n",
    "if dataset == \"fashion_mnist\":\n",
    "  fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "  (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "elif dataset == \"mnist\":\n",
    "  mnist = tf.keras.datasets.mnist\n",
    "  (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "  class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "elif dataset == \"cifar\":\n",
    "  (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "  class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "  train_labels = train_labels.squeeze()\n",
    "  test_labels = test_labels.squeeze()\n",
    "\n",
    "input_shape = train_images.shape[1:]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "print(\"Training Images range from {:2.5f} to {:2.5f}\".format(np.min(train_images), np.max(train_images)))\n",
    "print(\"Test     Images range from {:2.5f} to {:2.5f}\".format(np.min(test_images), np.max(test_images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vanilla-resort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print (input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "earlier-liability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              16778240  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 16,807,882\n",
      "Trainable params: 16,807,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    layers.Conv2D(32, kernel_size=(3,3), activation=\"relu\", padding='same'),\n",
    "    layers.Conv2D(64, kernel_size=(3,3), activation=\"relu\", padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "correct-representative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 191s 122ms/step - loss: 1.5141 - accuracy: 0.4572\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 184s 118ms/step - loss: 0.8554 - accuracy: 0.7009\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.5438 - accuracy: 0.8122\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 185s 118ms/step - loss: 0.2628 - accuracy: 0.9112\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 186s 119ms/step - loss: 0.1074 - accuracy: 0.9656\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 184s 118ms/step - loss: 0.0678 - accuracy: 0.9792\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 185s 118ms/step - loss: 0.0509 - accuracy: 0.9843\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 184s 118ms/step - loss: 0.0502 - accuracy: 0.9842\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 181s 116ms/step - loss: 0.0446 - accuracy: 0.9857\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 199s 127ms/step - loss: 0.0417 - accuracy: 0.9869\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 215s 137ms/step - loss: 0.0405 - accuracy: 0.9872\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 219s 140ms/step - loss: 0.0340 - accuracy: 0.9891\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 198s 126ms/step - loss: 0.0316 - accuracy: 0.9904\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 207s 133ms/step - loss: 0.0302 - accuracy: 0.9905\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 189s 121ms/step - loss: 0.0343 - accuracy: 0.9892\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 187s 120ms/step - loss: 0.0302 - accuracy: 0.9901\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 188s 120ms/step - loss: 0.0335 - accuracy: 0.9898\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 186s 119ms/step - loss: 0.0304 - accuracy: 0.9904\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 206s 132ms/step - loss: 0.0221 - accuracy: 0.9931\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.0294 - accuracy: 0.9916\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 219s 140ms/step - loss: 0.0296 - accuracy: 0.9915\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.0272 - accuracy: 0.9917\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 206s 132ms/step - loss: 0.0256 - accuracy: 0.9923\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 206s 132ms/step - loss: 0.0281 - accuracy: 0.9911\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 197s 126ms/step - loss: 0.0254 - accuracy: 0.9931\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 191s 122ms/step - loss: 0.0313 - accuracy: 0.9910\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 198s 127ms/step - loss: 0.0238 - accuracy: 0.9937\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 199s 127ms/step - loss: 0.0289 - accuracy: 0.9918\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 198s 126ms/step - loss: 0.0227 - accuracy: 0.9936\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 196s 125ms/step - loss: 0.0260 - accuracy: 0.9929\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 190s 122ms/step - loss: 0.0237 - accuracy: 0.9936\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 193s 123ms/step - loss: 0.0235 - accuracy: 0.9934\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 197s 126ms/step - loss: 0.0230 - accuracy: 0.9939\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.0227 - accuracy: 0.9942\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 199s 127ms/step - loss: 0.0274 - accuracy: 0.9935\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.0197 - accuracy: 0.9951\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 205s 131ms/step - loss: 0.0228 - accuracy: 0.9940\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 199s 127ms/step - loss: 0.0268 - accuracy: 0.9934\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 192s 123ms/step - loss: 0.0237 - accuracy: 0.9947\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 204s 130ms/step - loss: 0.0267 - accuracy: 0.9934\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 188s 120ms/step - loss: 0.0134 - accuracy: 0.9964\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 192s 123ms/step - loss: 0.0231 - accuracy: 0.9941\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 180s 115ms/step - loss: 0.0202 - accuracy: 0.9949\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 179s 115ms/step - loss: 0.0193 - accuracy: 0.9957\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 178s 114ms/step - loss: 0.0200 - accuracy: 0.9959\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 182s 117ms/step - loss: 0.0223 - accuracy: 0.9945\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 184s 118ms/step - loss: 0.0221 - accuracy: 0.9957\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 185s 118ms/step - loss: 0.0219 - accuracy: 0.9951\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 186s 119ms/step - loss: 0.0176 - accuracy: 0.9956\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 185s 118ms/step - loss: 0.0158 - accuracy: 0.9963\n",
      "INFO:tensorflow:Assets written to: saved_models/cifar_cnn_model/assets\n"
     ]
    }
   ],
   "source": [
    "train_hist = model.fit(train_images, train_labels, epochs=50)\n",
    "model.save('saved_models/cifar_cnn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polar-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 5s - loss: 5.7021 - accuracy: 0.6645\n",
      "\n",
      "Test accuracy: 0.6644999980926514\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "skilled-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizing to 32x32\n",
    "im = Image.open('/Users/Amanda/Desktop/airplane.jpg')\n",
    "newsize = (32, 32) \n",
    "resized_im = im.resize(newsize)   \n",
    "resized_im.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "immediate-pound",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'PIL.Image.Image'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-3be05286c7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_im\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_im\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[1;32m   1597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1599\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    959\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    962\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \"input: {}, {}\".format(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'PIL.Image.Image'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "#processing\n",
    "im = (np.expand_dims(resized_im,0)) #batch\n",
    "prediction = model.predict(resized_im) #process\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "np.argmax(prediction[0]) #\"grab\" prediction out of list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "promotional-dialogue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "depthwise_conv2d (DepthwiseC (None, 16, 16, 3)         30        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 8, 8, 32)          320       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          2112      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,062,440\n",
      "Trainable params: 1,062,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = tf.keras.Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    layers.DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding='same'),\n",
    "    layers.Conv2D(32, kernel_size=(1,1), strides=(1,1), activation=\"relu\", padding='same'),\n",
    "    layers.DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding='same'),\n",
    "    layers.Conv2D(64, kernel_size=(1,1), strides=(1,1), activation=\"relu\", padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "]) \n",
    "\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-violin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
